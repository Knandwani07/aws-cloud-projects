# ğŸ§¹ Data Cleaning & Transformation with AWS Glue DataBrew  
## ğŸ“Œ Project Level: Intermediate  

A practical AWS data engineering project where you clean, transform, and prepare datasets using **AWS Glue DataBrew** a visual, no-code tool designed for data wrangling.  
This project demonstrates how to automate data quality improvements, remove inconsistencies, perform transformations, and export cleaned datasets for analytics.

Using a sample dataset, you apply multiple DataBrew recipe steps to prepare the data and publish the cleaned output to an S3 bucket.

---

## ğŸ“ Project Overview  
This project walks through creating a **DataBrew project**, connecting to an **S3 dataset**, applying recipe steps, and running a **DataBrew job** to generate a clean, ready-to-use dataset.

You perform tasks like removing null values, changing column types, normalizing formats, splitting fields, filtering records, and applying transformations needed for analytics pipelines.

The final output is delivered back into S3 as a cleaned CSV file fully automated and ready for further BI or ML use cases.

---

## ğŸ¯ Objective  
To clean, transform, and prepare raw data for analytics using AWS Glue DataBrew by applying recipe-based transformations and publishing the processed dataset into an S3 bucket.

---

## ğŸ§° AWS Services Used  
- **AWS Glue DataBrew** â€” Visual data cleaning and transformation  
- **Amazon S3** â€” Source and output storage for datasets  
- **AWS IAM** â€” Permissions for dataset access and job execution  
- **DataBrew Jobs** â€” Automated processing and output generation  

---

## ğŸ§  What This Project Teaches  
- Creating a DataBrew project connected to S3  
- Understanding datasets, profiles, and recipes  
- Cleaning and transforming data with no-code operations  
- Applying recipe steps such as:
  - Removing empty or invalid rows  
  - Changing column data types  
  - Standardizing formats (dates, strings, numbers)  
  - Splitting and merging columns  
  - Filtering and sorting  
  - Handling missing values  
- Running a DataBrew job to automate the transformation  
- Exporting cleaned data back into S3  

---

## ğŸš€ Project Steps (Simplified)

1. Upload the raw dataset into an S3 bucket.  
2. Open AWS Glue DataBrew and create a new project.  
3. Choose the S3 dataset as your source.  
4. Begin applying recipe steps:
   - Format standardization  
   - Null value handling  
   - Column cleanup and data type corrections  
   - Transformations based on dataset needs  
5. Save the recipe and create a **DataBrew Job**.  
6. Run the job to generate the cleaned dataset.  
7. Verify the output in S3 and review the transformation results.

Full detailed steps with screenshots are available in the project documentation.

---

## ğŸŒŸ Key Features  
- Visual, no-code data cleaning  
- Recipe-based repeatable transformations  
- Automated data preparation workflows  
- Clean, structured output ready for QuickSight, Athena, ML models, or analytics pipelines  
- Real-world data engineering workflow demonstration  

---

## ğŸ§¹ Cleanup  
To avoid additional costs:

- Delete the DataBrew job  
- Delete the DataBrew project  
- Remove recipe versions no longer needed  
- Delete output files from S3 (optional)  

---

## ğŸ Outcome  
By completing this project, you gain hands-on experience with **data wrangling, transformation, and preparation** using AWS Glue DataBrew.  
You learn how to convert messy raw datasets into analytics-ready clean dataâ€”an essential skill in data engineering and machine learning workflows.

---

## ğŸ¥ Project Demo Video  
ğŸ‘‰ https://www.linkedin.com/posts/khushi-nandwani_devopschronicles-episode13-awsglue-activity-7318868400422735872-zRus  

---

## ğŸ“„ Full Project Documentation  
ğŸ‘‰ https://www.linkedin.com/posts/khushi-nandwani_aws-glue-databrew-project-documentation-activity-7319230787709489152-N-Q0  

---

## ğŸ·ï¸ Tags  
`AWS Glue DataBrew` `Data Cleaning` `Data Transformation` `ETL`  
`Amazon S3` `Data Engineering` `Intermediate AWS Project`

